{
  "results": [
    {
      "question_id": "rq_001",
      "query": "What are the current leading approaches to quantum error correction and how do they compare in terms of overhead and fault-tolerance thresholds?",
      "system": "flashresearch",
      "time_budget_s": 120,
      "elapsed_time": 27.93416976928711,
      "node_count": 5,
      "findings_count": 25,
      "report_length": 4031,
      "scores": {
        "quality": 85.0,
        "relevance": 80.0,
        "faithfulness": 75.0,
        "overall": 80.0,
        "explanation": "The report is well-organized with clear sections and logical flow, though it could benefit from more detailed citations and a slightly more professional formatting. It addresses the query by discussing leading approaches like surface and topological codes, but lacks depth in comparing overhead and fault-tolerance thresholds. Most claims are supported by the context, but some details, such as specific comparisons of overhead and fault-tolerance, are not fully substantiated by the provided sources."
      },
      "report": "# Quantum Error Correction: Current Approaches and Comparisons\n\n## Executive Summary\nQuantum error correction (QEC) has evolved significantly, with leading approaches such as surface codes and topological codes emerging as frontrunners for scalable quantum computing. These methods vary in overhead and fault-tolerance thresholds, with surface codes showing promise for practical implementation by 2025, while challenges remain in achieving efficient error correction with minimal resource requirements.\n\n## Key Findings\n- **Surface Codes**: These codes are pivotal for scalable quantum error correction, enabling below-threshold logical qubits with improved fidelity in superconducting systems, making them a leading approach for practical quantum computing by 2025 [Quantum error correction - Wikipedia](https://en.wikipedia.org/wiki/Quantum_error_correction).\n- **Topological Codes**: Recent advancements have demonstrated entangling gates between logical qubits encoded in topological codes, marking a significant step towards fault-tolerant quantum computing [The Decade of Fault-Tolerant Quantum Computing](https://www.preprints.org/manuscript/202509.2238).\n- **Resource Overhead**: QEC requires significant overhead in terms of additional qubits and gates, with the need for precise control to detect and correct errors without disturbing quantum states [What is Quantum Error Correction (QEC) - QuEra](https://www.quera.com/glossary/quantum-error-correction).\n- **Scalability**: Novel error correction codes are being developed that can handle hundreds of thousands of qubits, potentially enabling large-scale fault-tolerant quantum systems [Quantum error correction codes enable efficient scaling to hundreds](https://phys.org/news/2025-09-quantum-error-codes-enable-efficient.html).\n- **Fault-Tolerance Thresholds**: The transition from theoretical to experimental QEC has led to the realization of below-threshold logical qubits, indicating that fault-tolerance is becoming a concrete engineering challenge rather than a distant aspiration [The Decade of Fault-Tolerant Quantum Computing](https://www.preprints.org/manuscript/202509.2238).\n\n## Analysis\nThe current landscape of quantum error correction is characterized by a variety of approaches, each with its strengths and weaknesses. Surface codes are particularly notable for their balance of overhead and fault-tolerance, making them suitable for near-term implementations. In contrast, while topological codes offer robust error correction capabilities, they may involve higher complexity and resource demands.\n\nDespite the advancements, challenges persist, particularly regarding the overhead associated with QEC. The need for additional qubits and gates can complicate the scalability of quantum systems. Furthermore, while some codes have demonstrated low overhead, they often suffer from low encoding efficiency, which can hinder their practical application [Demonstration of low-overhead quantum error correction codes](https://www.researchgate.net/publication/391776188_Demonstration_of_low-overhead_quantum_error_correction_codes).\n\nThere is also a notable gap in understanding the trade-offs between different QEC approaches, particularly in terms of their fault-tolerance thresholds and practical implementation challenges. As research progresses, it will be crucial to evaluate how these methods can be optimized for real-world applications.\n\n## Conclusion\nThe field of quantum error correction is rapidly advancing, with surface and topological codes leading the way in terms of practical implementation and fault-tolerance. However, the significant overhead associated with these methods poses challenges for scalability. As researchers continue to explore and refine QEC techniques, understanding the trade-offs between overhead and fault-tolerance will be essential for the future of quantum computing. Current evidence highlights a promising trajectory, yet gaps remain in the practical application and efficiency of these error correction methods."
    },
    {
      "question_id": "rq_001",
      "query": "What are the current leading approaches to quantum error correction and how do they compare in terms of overhead and fault-tolerance thresholds?",
      "system": "baseline",
      "time_budget_s": 120,
      "elapsed_time": 10.785276174545288,
      "node_count": 1,
      "findings_count": 5,
      "report_length": 3780,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 80.0,
        "overall": 85.0,
        "explanation": "The report is well-structured with clear sections and logical flow, though it could benefit from more detailed citations and professional formatting. It directly addresses the query by discussing leading QEC approaches and comparing them in terms of overhead and fault-tolerance thresholds. However, some claims, particularly about state-adaptive QEC and specific overhead comparisons, lack direct support from the provided context, indicating minor faithfulness issues."
      },
      "report": "# Research Report: Current Leading Approaches to Quantum Error Correction\n\n## Executive Summary\n\nQuantum error correction (QEC) is a critical component in the development of fault-tolerant quantum computing. As quantum systems become more complex, the need for robust error correction methods has intensified. This report examines the current leading approaches to quantum error correction, focusing on their overhead requirements and fault-tolerance thresholds. The analysis draws on recent trends and predictions from industry experts, highlighting the transition to new coding schemes and the implications for future quantum computing architectures.\n\n## Key Findings\n\n1. **Leading Approaches to Quantum Error Correction**:\n   - **Surface Codes**: Widely regarded as the most promising QEC method, surface codes offer a high fault-tolerance threshold (approximately 1%) and relatively low overhead in terms of physical qubits required for logical qubits. They are particularly well-suited for two-dimensional qubit architectures.\n   - **Quantum Low-Density Parity-Check (qLDPC) Codes**: Following IBM's adoption of qLDPC codes in 2024, these codes are expected to gain traction across the industry. They provide a flexible framework for error correction, with a fault-tolerance threshold that can be competitive with surface codes, depending on implementation.\n   - **State-Adaptive QEC**: This emerging approach adapts the error correction strategy based on the state of the quantum system, potentially improving efficiency and reducing overhead. Preliminary results indicate that state-adaptive methods can enhance fault tolerance beyond traditional static codes.\n\n2. **Overhead Comparison**:\n   - **Physical Qubit Overhead**: Surface codes typically require a larger number of physical qubits to encode a logical qubit compared to qLDPC codes. However, the overhead can be mitigated through advancements in qubit technology and error rates.\n   - **Time Overhead**: The time required for error correction cycles can vary significantly between methods. Surface codes may require more time due to their complex error correction protocols, while qLDPC codes can offer faster correction times at the cost of increased physical qubit usage.\n\n3. **Fault-Tolerance Thresholds**:\n   - The fault-tolerance threshold is a critical metric for evaluating QEC methods. Surface codes maintain a threshold around 1%, making them robust against a range of error rates. qLDPC codes are expected to achieve similar thresholds, but their performance is highly dependent on the specific implementation and error models used.\n   - State-adaptive QEC shows promise for improving fault tolerance, but further research is needed to quantify its thresholds in practical applications.\n\n4. **Industry Trends**:\n   - The shift towards qLDPC codes is indicative of a broader trend in the quantum computing industry, where diverse error correction strategies are being explored. This diversification is expected to lead to more resilient quantum systems capable of operating in real-world conditions.\n\n## Conclusion\n\nQuantum error correction remains a pivotal area of research in the quest for fault-tolerant quantum computing. Current leading approaches, including surface codes, qLDPC codes, and state-adaptive methods, each present unique advantages and challenges in terms of overhead and fault-tolerance thresholds. As the industry evolves, the adoption of qLDPC codes and the exploration of adaptive strategies may redefine the landscape of quantum error correction, paving the way for more robust quantum computing systems. Continued research and development in this field will be essential to overcome the challenges posed by quantum noise and to realize the full potential of quantum technologies."
    },
    {
      "question_id": "rq_002",
      "query": "How has CRISPR-Cas9 technology evolved since 2020, and what are the key regulatory and ethical debates surrounding its clinical applications?",
      "system": "flashresearch",
      "time_budget_s": 120,
      "elapsed_time": 25.575781106948853,
      "node_count": 5,
      "findings_count": 25,
      "report_length": 4283,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 95.0,
        "overall": 90.0,
        "explanation": "The report is well-organized with clear sections and logical flow, though it could benefit from more detailed citations and professional formatting. It directly addresses the query about CRISPR-Cas9 advancements and regulatory/ethical debates, covering key points comprehensively. The claims are well-supported by the provided context, with no significant hallucinations, though minor details could be more explicitly linked to sources."
      },
      "report": "## Executive Summary\nSince 2020, CRISPR-Cas9 technology has seen significant advancements, including the first approval of a CRISPR-based therapy for sickle cell disease in late 2023. However, the rapid evolution of this technology has sparked ongoing regulatory and ethical debates, particularly concerning its applications in human germline editing and the potential for exacerbating social inequalities.\n\n## Key Findings\n- **Clinical Advancements**: The first CRISPR-based medicine, Casgevy, was approved in late 2023 for treating sickle cell disease and transfusion-dependent beta thalassemia, marking a significant milestone in gene editing therapies ([Innovative Genomics Institute](https://innovativegenomics.org/news/crispr-clinical-trials-2024/)).\n- **Technological Innovations**: Advances in CRISPR platforms (Cas9, Cas12, Cas13) have improved precision and versatility, enhancing therapeutic applications and reducing off-target effects ([Recent Advancements in CRISPR-Cas9 Technology for Precision Gene Editing](https://www.jneonatalsurg.com/index.php/jns/article/view/6384)).\n- **Regulatory Developments**: Countries like China have updated their regulatory frameworks for human genome editing, although challenges remain in ensuring comprehensive oversight ([Regulatory framework of human germline and heritable](https://pmc.ncbi.nlm.nih.gov/articles/PMC12069028/)).\n- **Ethical Concerns**: The potential for off-target effects raises significant ethical questions regarding the safety of CRISPR applications in humans, with calls for stricter regulations and ethical guidelines ([Ethical and Scientific Concerns Relating to CRISPR/Cas Gene Therapy](https://criticaldebateshsgj.scholasticahq.com/article/144179-ethical-and-scientific-concerns-relating-to-crispr-cas-gene-therapy)).\n- **Social Implications**: The integration of CRISPR technology into society poses risks of exacerbating social inequalities, particularly in access to gene therapies and the potential for misuse ([Ethical dimensions and societal implications: ensuring the social](https://www.frontiersin.org/journals/genome-editing/articles/10.3389/fgeed.2025.1593172/full)).\n\n## Analysis\nThe evolution of CRISPR-Cas9 technology since 2020 has been marked by both remarkable progress and significant challenges. The approval of Casgevy represents a breakthrough in the clinical application of gene editing, showcasing the potential for CRISPR to address previously untreatable genetic disorders. However, the rapid pace of innovation has outstripped the development of regulatory frameworks, leading to a patchwork of guidelines that vary significantly across countries. For instance, while China has made strides in updating its regulations, other regions lag behind, creating inconsistencies in the oversight of gene editing practices ([Regulatory framework of human germline and heritable](https://pmc.ncbi.nlm.nih.gov/articles/PMC12069028/)).\n\nEthically, the debates surrounding CRISPR are multifaceted. Concerns about off-target effects and the long-term implications of germline editing remain at the forefront of discussions, with many experts advocating for a cautious approach to human applications ([Ethical and Scientific Concerns Relating to CRISPR/Cas Gene Therapy](https://criticaldebateshsgj.scholasticahq.com/article/144179-ethical-and-scientific-concerns-relating-to-crispr-cas-gene-therapy)). Additionally, the potential for CRISPR to exacerbate existing social inequalities raises critical questions about equitable access to these technologies and the ethical responsibilities of researchers and policymakers ([Ethical dimensions and societal implications: ensuring the social](https://www.frontiersin.org/journals/genome-editing/articles/10.3389/fgeed.2025.1593172/full)).\n\n## Conclusion\nThe advancements in CRISPR-Cas9 technology since 2020 highlight its transformative potential in medicine, particularly in treating genetic disorders. However, the accompanying regulatory and ethical challenges underscore the need for a balanced approach that prioritizes safety, equity, and societal implications. As the field continues to evolve, addressing these gaps in regulation and ethical considerations will be crucial to harnessing the full potential of CRISPR technology responsibly."
    },
    {
      "question_id": "rq_002",
      "query": "How has CRISPR-Cas9 technology evolved since 2020, and what are the key regulatory and ethical debates surrounding its clinical applications?",
      "system": "baseline",
      "time_budget_s": 120,
      "elapsed_time": 13.954180240631104,
      "node_count": 1,
      "findings_count": 5,
      "report_length": 4048,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 95.0,
        "overall": 90.0,
        "explanation": "The report is well-structured with clear sections and logical flow, though it could benefit from more detailed citations. It directly addresses the query by discussing advancements in CRISPR-Cas9 since 2020 and the associated regulatory and ethical debates. The information is largely supported by the provided context, with minor gaps in specific details about regulatory frameworks and ethical debates."
      },
      "report": "# Research Report: Evolution of CRISPR-Cas9 Technology and Regulatory and Ethical Debates Surrounding Clinical Applications\n\n## Executive Summary\n\nSince 2020, CRISPR-Cas9 technology has undergone significant advancements, expanding its applications in therapeutic and biomedical fields. As of early 2025, approximately 250 clinical trials involving CRISPR-based therapies are underway, with over 150 focused on various genetic disorders and cancers. However, the rapid evolution of this technology has sparked critical regulatory and ethical debates, particularly concerning safety, efficacy, and the implications of germline editing. This report synthesizes recent advancements in CRISPR-Cas9 technology and outlines the key regulatory and ethical challenges that accompany its clinical applications.\n\n## Key Findings\n\n### 1. Advances in CRISPR-Cas9 Technology\n- **Therapeutic Applications**: The CRISPR-Cas9 system has been increasingly utilized in therapeutic contexts, particularly for genetic disorders, cancers, and infectious diseases. Recent studies highlight its transformative impact on gene therapy, with promising results in preclinical and early-phase clinical trials (Frontiers, 2024).\n- **Clinical Trials**: As of February 2025, there are approximately 250 clinical trials involving CRISPR technology, with a significant number targeting genetic disorders and cancers. The first in vivo clinical trial was approved in 2019, marking a pivotal moment in the application of CRISPR for direct patient treatment (CRISPR Medicine News, 2025).\n- **Technological Enhancements**: Innovations such as base editing and prime editing have emerged, offering more precise editing capabilities with reduced off-target effects. These advancements are crucial for enhancing the safety profile of CRISPR applications (NIH, 2021).\n\n### 2. Regulatory Landscape\n- **Regulatory Frameworks**: Regulatory bodies worldwide are grappling with how to effectively oversee CRISPR technologies. The U.S. Food and Drug Administration (FDA) and the European Medicines Agency (EMA) have begun to establish guidelines for the clinical use of gene-editing technologies, focusing on safety and ethical considerations (CBHD, 2023).\n- **Germline Editing Regulations**: The potential for germline editing raises significant regulatory challenges. Many countries have imposed strict regulations or outright bans on germline modifications due to ethical concerns about long-term implications and the potential for \"designer babies.\"\n\n### 3. Ethical Debates\n- **Safety and Efficacy**: The primary ethical concern surrounding CRISPR technology is the safety and long-term efficacy of gene editing. Off-target effects and unintended consequences pose risks that necessitate thorough evaluation before widespread clinical use (PMC, 2021).\n- **Equity and Access**: There are concerns about equitable access to CRISPR therapies, particularly in low- and middle-income countries. The potential for disparities in access to advanced genetic therapies raises ethical questions about fairness and justice in healthcare (PMC, 2021).\n- **Public Perception and Informed Consent**: The rapid pace of CRISPR advancements has outstripped public understanding and regulatory frameworks. Ensuring informed consent and addressing public concerns about genetic modifications are critical for the ethical deployment of CRISPR technologies (CBHD, 2023).\n\n## Conclusion\n\nThe evolution of CRISPR-Cas9 technology since 2020 has been marked by significant advancements in therapeutic applications and clinical trials. However, the rapid pace of development has also led to complex regulatory and ethical challenges. As the scientific community continues to explore the potential of CRISPR, it is imperative to establish robust regulatory frameworks and engage in ongoing ethical discussions to ensure that the benefits of this transformative technology are realized safely and equitably. Addressing these challenges will be crucial for the responsible advancement of CRISPR-Cas9 in clinical settings."
    },
    {
      "question_id": "rq_003",
      "query": "What are the main architectural differences between transformer models and state space models, and what are the tradeoffs for long-context tasks?",
      "system": "flashresearch",
      "time_budget_s": 120,
      "elapsed_time": 24.10282325744629,
      "node_count": 5,
      "findings_count": 25,
      "report_length": 3604,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 95.0,
        "overall": 90.0,
        "explanation": "The report is well-structured with clear sections and logical flow, though it could benefit from more detailed citations and professional formatting for a higher quality score. It directly addresses the query about architectural differences and trade-offs for long-context tasks, making it highly relevant. The claims made in the report are well-supported by the provided context, with no significant hallucinations, ensuring high faithfulness."
      },
      "report": "## Executive Summary\nThis report examines the architectural differences between transformer models and state space models (SSMs), highlighting their respective strengths and weaknesses in handling long-context tasks. Key findings indicate that while transformers excel in attention-based processing, SSMs offer significant computational advantages, particularly in terms of scalability and efficiency for long sequences.\n\n## Key Findings\n- **Architectural Differences**: Transformers utilize attention mechanisms to process input sequences, leading to quadratic scaling in computational complexity, while SSMs maintain a continuous state that updates over time, resulting in linear O(N) complexity for sequence processing [From Transformers to State Space Models](https://www.deeper-thoughts-blog.rohithshinoj.com/blog/state-space-models).\n- **Scalability**: SSMs are more scalable than transformers when dealing with long sequences, as they circumvent the quadratic scaling issue inherent in transformers' self-attention mechanisms [Exploring the Future of LLMs: State-Space Models vs. Transformers](https://www.linkedin.com/pulse/exploring-future-llms-state-space-models-vs-jack-alexander-qwxyc).\n- **Computational Efficiency**: Research shows that SSMs demonstrate superior memory usage and inference speed compared to transformers, especially as the context length increases from 512 to 8,192 tokens [Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers](https://arxiv.org/pdf/2601.01237).\n- **Long-Context Reasoning**: SSMs have unlocked new capabilities in long-context reasoning, addressing the limitations faced by transformers in this area [From Transformers to State Space Models](https://www.deeper-thoughts-blog.rohithshinoj.com/blog/state-space-models).\n\n## Analysis\nThe architectural distinctions between transformers and SSMs fundamentally influence their performance on long-context tasks. Transformers, with their reliance on attention mechanisms, excel in capturing relationships within data but suffer from inefficiencies as the input size grows. This quadratic scaling can lead to significant computational overhead, making them less suitable for applications requiring extensive context.\n\nIn contrast, SSMs leverage a continuous state representation that allows for efficient updates and processing, resulting in linear scaling. This characteristic not only enhances their computational efficiency but also enables them to handle longer sequences more effectively. However, the trade-off may involve a potential loss in the nuanced attention capabilities that transformers provide, particularly in scenarios where fine-grained relationships within data are crucial.\n\nDespite the advantages of SSMs, there remains some uncertainty regarding their overall representational power compared to transformers. While SSMs are promising for long-context tasks, further research is needed to fully understand their limitations and potential applications in various domains.\n\n## Conclusion\nThe architectural differences between transformer models and state space models reveal significant implications for their use in long-context tasks. SSMs present a compelling alternative to transformers, particularly in terms of computational efficiency and scalability. However, the trade-offs in representational power and the nuances of attention mechanisms warrant further investigation. As research continues to evolve, understanding these models' strengths and weaknesses will be crucial for optimizing their application in real-world scenarios."
    },
    {
      "question_id": "rq_003",
      "query": "What are the main architectural differences between transformer models and state space models, and what are the tradeoffs for long-context tasks?",
      "system": "baseline",
      "time_budget_s": 120,
      "elapsed_time": 14.169835805892944,
      "node_count": 1,
      "findings_count": 5,
      "report_length": 3978,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 80.0,
        "overall": 85.0,
        "explanation": "The report is well-structured with clear sections and logical flow, though it could benefit from more detailed citations. It directly addresses the query by comparing architectural differences and trade-offs for long-context tasks. Most claims are supported by the provided context, but some details about hybrid models and specific performance metrics could use more explicit sourcing."
      },
      "report": "# Research Report: Architectural Differences Between Transformer Models and State Space Models\n\n## Executive Summary\n\nThis report explores the architectural differences between Transformer models and State Space Models (SSMs), focusing on their implications for long-context tasks. Transformers, characterized by their attention mechanisms, excel in capturing relationships across extensive input sequences. In contrast, SSMs utilize a constant hidden state to model sequences, offering linear efficiency in processing. The trade-offs between these architectures manifest in their performance on tasks requiring long-context understanding, with Transformers generally outperforming SSMs in tasks like copying from input context. However, hybrid models that integrate both architectures are emerging, promising to leverage the strengths of each while mitigating their weaknesses.\n\n## Key Findings\n\n1. **Architectural Differences**:\n   - **Transformers**:\n     - Utilize self-attention mechanisms to weigh the importance of different input tokens, allowing for the capture of long-range dependencies.\n     - Require quadratic time complexity concerning the input length due to the pairwise attention calculations.\n     - Are highly parallelizable, making them efficient during training despite their computational demands.\n   - **State Space Models (SSMs)**:\n     - Represent sequences through a constant hidden state, which evolves over time based on input, leading to linear time complexity.\n     - Are designed to model dynamic systems and can efficiently handle long sequences without the quadratic overhead of attention mechanisms.\n     - Lack the explicit attention mechanism, which can limit their ability to capture complex relationships in data.\n\n2. **Performance on Long-Context Tasks**:\n   - Research indicates that Transformers outperform SSMs in tasks requiring the retention and copying of long sequences, as demonstrated in the study \"Repeat After Me: Transformers are Better than State Space Models at Copying from their Input Context.\"\n   - SSMs, while efficient, may struggle with tasks that require nuanced understanding of relationships across long contexts due to their reliance on a constant hidden state.\n\n3. **Emerging Hybrid Architectures**:\n   - Hybrid models that combine the attention mechanisms of Transformers with the efficiency of SSMs are gaining traction. These architectures aim to provide the best of both worlds, potentially enhancing performance on long-context tasks while maintaining computational efficiency.\n   - The paper \"End of Transformers? Attention + State-Space Hybrids in 2025\" discusses the potential of these hybrid models to outperform traditional architectures in specific applications.\n\n4. **Theoretical Trade-offs**:\n   - The theoretical underpinnings of both architectures reveal distinct advantages and disadvantages. While Transformers excel in tasks requiring complex relational understanding, SSMs offer a more scalable solution for processing long sequences.\n   - The comparative analysis in \"The New Wave of Sequence Modeling\" highlights these trade-offs, suggesting that the choice of architecture should be informed by the specific requirements of the task at hand.\n\n## Conclusion\n\nThe architectural differences between Transformer models and State Space Models present significant implications for their application in long-context tasks. While Transformers demonstrate superior performance in capturing complex relationships across extensive input sequences, SSMs offer a more efficient alternative for processing long sequences. The emergence of hybrid architectures that integrate the strengths of both models may pave the way for advancements in sequence modeling, balancing the trade-offs between computational efficiency and performance. Future research should continue to explore these hybrid models, as they hold promise for addressing the limitations of both Transformers and SSMs in various applications."
    },
    {
      "question_id": "rq_001",
      "query": "What are the current leading approaches to quantum error correction and how do they compare in terms of overhead and fault-tolerance thresholds?",
      "system": "flashresearch",
      "time_budget_s": 600,
      "elapsed_time": 23.7106990814209,
      "node_count": 5,
      "findings_count": 24,
      "report_length": 3576,
      "scores": {
        "quality": 85.0,
        "relevance": 80.0,
        "faithfulness": 75.0,
        "overall": 80.0,
        "explanation": "The report is well-organized with clear sections and logical flow, but it could benefit from more detailed citations and a slightly more professional formatting. It addresses the query well, discussing leading approaches like surface codes and topological codes, but lacks depth in comparing overhead and fault-tolerance thresholds. Most claims are supported by the context, but there are some generalizations and missing details that affect faithfulness."
      },
      "report": "# Current Approaches to Quantum Error Correction: A Comprehensive Report\n\n## Executive Summary\nQuantum error correction (QEC) is crucial for the development of fault-tolerant quantum computing. Leading approaches, including surface codes and topological codes, show promise in balancing overhead and fault-tolerance thresholds, with surface codes emerging as particularly advantageous for scalable quantum systems by 2025.\n\n## Key Findings\n- **Surface Codes**: These codes are pivotal for scalable quantum error correction, enabling below-threshold logical qubits with improved fidelity in superconducting systems. They are expected to be a leading approach by 2025 due to their relatively low overhead and high fault-tolerance thresholds [Quantum error correction](https://en.wikipedia.org/wiki/Quantum_error_correction).\n- **Topological Codes**: These codes, such as the ones based on the toric code, provide inherent protection against local errors and are characterized by their robustness. They are considered fault-tolerant but may require higher overhead compared to surface codes [Quantum Errors and Quantum Error Correction (QEC) Methods](https://postquantum.com/quantum-computing/quantum-error-correction/).\n- **Bivariate Bicycle Codes**: IBM's recent work on bivariate bicycle codes offers a modular framework for fault-tolerant quantum computing, which could potentially lower overhead while maintaining high fault-tolerance [IBM lays out clear path to fault-tolerant quantum computing](https://www.ibm.com/quantum/blog/large-scale-ftqc).\n- **Error Suppression and Detection**: Complementary techniques such as error suppression, detection, and mitigation are essential for achieving quantum robustness and enhancing the performance of QEC methods [Making fault-tolerant quantum computers a reality - McKinsey](https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/tech-forward/making-fault-tolerant-quantum-computers-a-reality).\n- **Practical Challenges**: Despite advancements, challenges remain regarding qubit fidelity, error thresholds, and resource overhead, which can impact the effectiveness of various QEC strategies [Quantum error correction : an introductory guide](https://iontrap.duke.edu/files/2025/03/arxiv_sub_v2.pdf).\n\n## Analysis\nThe landscape of quantum error correction is diverse, with multiple approaches offering unique advantages and trade-offs. Surface codes are favored for their scalability and lower overhead, making them suitable for near-term quantum computing applications. In contrast, topological codes, while robust, may require more resources, which could hinder their immediate applicability. The introduction of bivariate bicycle codes by IBM represents a significant innovation, potentially bridging the gap between overhead and fault tolerance. However, the practical implementation of these codes still faces hurdles, particularly in maintaining qubit fidelity and managing error thresholds. The integration of error suppression and detection techniques is crucial for enhancing the overall robustness of quantum systems.\n\n## Conclusion\nThe current state of quantum error correction reveals a promising yet complex landscape. Surface codes and topological codes are at the forefront, each with distinct advantages and challenges. While advancements like bivariate bicycle codes offer new pathways for fault tolerance, significant gaps remain in practical implementation and scalability. Continued research is essential to address these challenges and to refine QEC methods for future quantum computing applications."
    },
    {
      "question_id": "rq_001",
      "query": "What are the current leading approaches to quantum error correction and how do they compare in terms of overhead and fault-tolerance thresholds?",
      "system": "baseline",
      "time_budget_s": 600,
      "elapsed_time": 18.508477210998535,
      "node_count": 1,
      "findings_count": 5,
      "report_length": 4156,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 80.0,
        "overall": 85.0,
        "explanation": "The report is well-structured with clear sections and logical flow, though it could benefit from more detailed citations and professional formatting. It directly addresses the query by comparing leading QEC approaches in terms of overhead and fault-tolerance thresholds, covering both stabilizer and LDPC codes. However, some claims, particularly about new frameworks and industry trends, lack explicit support from the provided context, indicating minor faithfulness issues."
      },
      "report": "# Research Report: Current Leading Approaches to Quantum Error Correction\n\n## Executive Summary\n\nQuantum error correction (QEC) is a critical component in the development of fault-tolerant quantum computing. As quantum systems are inherently susceptible to errors due to decoherence and operational imperfections, effective QEC strategies are essential for the realization of practical quantum computers. This report examines the leading approaches to quantum error correction, focusing on their overhead and fault-tolerance thresholds. Key findings indicate that while traditional stabilizer codes, such as surface codes, remain prominent, newer methods like low-density parity-check (LDPC) codes are gaining traction due to their potential for lower overhead and higher fault-tolerance thresholds. The report synthesizes insights from recent research and industry trends to provide a comprehensive overview of the current landscape of quantum error correction.\n\n## Key Findings\n\n1. **Stabilizer Codes and Surface Codes**:\n   - Stabilizer codes, particularly surface codes, are widely regarded as the gold standard for QEC due to their high fault-tolerance thresholds (approximately 1% error rate) and relatively manageable overhead.\n   - Surface codes require a significant number of physical qubits to encode logical qubits, leading to a high overhead in terms of qubit resources.\n\n2. **Low-Density Parity-Check (LDPC) Codes**:\n   - LDPC codes have emerged as a promising alternative to traditional stabilizer codes. IBM's transition to qLDPC codes is indicative of a broader industry shift towards these codes.\n   - LDPC codes can achieve comparable or even superior fault-tolerance thresholds while potentially reducing the overhead associated with qubit usage.\n   - Recent research suggests that LDPC codes can provide high-threshold and low-overhead solutions for fault-tolerant quantum memory, making them an attractive option for future quantum computing architectures.\n\n3. **New Fault-Tolerance Frameworks**:\n   - Recent advancements, such as the framework developed by researchers from QuEra, Harvard, and Yale, aim to drastically reduce time overhead in QEC processes. This new approach could significantly enhance the efficiency of quantum computations by minimizing the time required for error correction.\n   - These frameworks often leverage innovative decoding strategies and circuit designs that optimize the performance of existing QEC codes.\n\n4. **Fundamental Thresholds**:\n   - Research has focused on establishing fundamental thresholds for realistic QEC circuits, independent of specific decoding strategies. This work provides a clearer understanding of the limits of current QEC methods and highlights areas for future improvement.\n\n5. **Comparative Analysis of Overhead and Fault-Tolerance**:\n   - While surface codes offer robust fault-tolerance, their overhead can be substantial, particularly for large-scale implementations.\n   - LDPC codes, on the other hand, present a more efficient use of resources, potentially allowing for a higher density of logical qubits per physical qubit.\n   - The choice of QEC strategy often depends on the specific requirements of the quantum computing application, including the acceptable levels of error and the available physical resources.\n\n## Conclusion\n\nThe field of quantum error correction is rapidly evolving, with significant advancements in both theoretical frameworks and practical implementations. While traditional stabilizer codes, particularly surface codes, continue to play a vital role in QEC, the emergence of LDPC codes and new fault-tolerance frameworks indicates a shift towards more efficient and scalable solutions. As the quantum computing landscape matures, ongoing research and development will be crucial in optimizing these approaches, ultimately paving the way for fault-tolerant quantum systems capable of performing complex computations reliably. The comparative analysis of overhead and fault-tolerance thresholds will guide future innovations in quantum error correction, ensuring that the next generation of quantum computers can achieve their full potential."
    },
    {
      "question_id": "rq_002",
      "query": "How has CRISPR-Cas9 technology evolved since 2020, and what are the key regulatory and ethical debates surrounding its clinical applications?",
      "system": "flashresearch",
      "time_budget_s": 600,
      "elapsed_time": 45.18693494796753,
      "node_count": 16,
      "findings_count": 66,
      "report_length": 3381,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 75.0,
        "overall": 83.33333333333333,
        "explanation": "The report is well-organized with clear sections and logical flow, though it could benefit from more detailed citations and professional formatting. It directly addresses the query about CRISPR-Cas9's evolution and the associated ethical and regulatory debates, providing a comprehensive overview. However, some claims, such as the FDA approval of CASGEVY\u2122, are not directly supported by the provided context, indicating minor faithfulness issues."
      },
      "report": "## Executive Summary\nSince 2020, CRISPR-Cas9 technology has seen significant advancements in therapeutic applications, including the FDA approval of the first CRISPR-based therapy for sickle cell disease. However, these developments have sparked ongoing regulatory and ethical debates, particularly concerning human embryo editing and the implications of gene editing on future generations.\n\n## Key Findings\n- **Therapeutic Advancements**: The first CRISPR-Cas9-based gene editing therapy, CASGEVY\u2122, received FDA approval in late 2023 for treating sickle cell disease, marking a milestone in clinical applications of the technology ([Frontiers](https://www.frontiersin.org/journals/genome-editing/articles/10.3389/fgeed.2025.1724291/full)).\n- **Clinical Trials Expansion**: As of 2025, numerous clinical trials are underway, exploring CRISPR's potential in various diseases, including genetic disorders and cancers, with a focus on both autologous and allogenic therapies ([Innovative Genomics Institute](https://innovativegenomics.org/news/crispr-clinical-trials-2025/)).\n- **Emerging Techniques**: New methodologies such as Prime Editing and Base Editing are being integrated with CRISPR-Cas9, enhancing precision and reducing off-target effects ([ResearchGate](https://www.researchgate.net/publication/393775562_CRISPR-Cas9_in_2025_Editing_Life_Ethics_and_the_Future_of_Medicine_Overview)).\n- **Ethical Concerns**: A systematic review highlighted significant ethical debates surrounding human embryo editing, focusing on the potential for heritable changes and the moral implications of such interventions ([Springer](https://link.springer.com/article/10.1007/s10730-024-09538-1)).\n- **Regulatory Challenges**: The rapid evolution of CRISPR technology has outpaced existing regulatory frameworks, raising concerns about safety, efficacy, and ethical considerations in gene editing ([NIH](https://pmc.ncbi.nlm.nih.gov/articles/PMC7336378/)).\n\n## Analysis\nThe evolution of CRISPR-Cas9 technology since 2020 has been marked by significant therapeutic breakthroughs and the introduction of advanced editing techniques. The approval of CASGEVY\u2122 represents a critical step in translating CRISPR research into clinical practice. However, the expansion of clinical trials and the introduction of new editing methods also raise complex regulatory and ethical issues. \n\nConflicting viewpoints exist regarding the editing of human embryos. Proponents argue for the potential to eliminate genetic diseases, while opponents raise concerns about unforeseen consequences and the moral implications of altering human genetics. The lack of comprehensive regulatory frameworks further complicates these discussions, as existing laws may not adequately address the rapid advancements in gene editing technologies.\n\n## Conclusion\nThe advancements in CRISPR-Cas9 technology since 2020 have significant implications for the future of medicine, particularly in gene therapy. However, the ongoing ethical and regulatory debates highlight critical gaps in our understanding and governance of these technologies. As CRISPR continues to evolve, it is essential to establish robust ethical guidelines and regulatory frameworks to ensure safe and responsible use in clinical applications. Addressing these gaps will be crucial in harnessing the full potential of CRISPR while safeguarding against its risks."
    },
    {
      "question_id": "rq_002",
      "query": "How has CRISPR-Cas9 technology evolved since 2020, and what are the key regulatory and ethical debates surrounding its clinical applications?",
      "system": "baseline",
      "time_budget_s": 600,
      "elapsed_time": 13.600910186767578,
      "node_count": 1,
      "findings_count": 5,
      "report_length": 3750,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 95.0,
        "overall": 90.0,
        "explanation": "The report is well-structured with clear sections and logical flow, though it could benefit from more detailed citations and slightly improved formatting. It directly and comprehensively addresses the query, covering both technological advancements and regulatory/ethical debates. The claims are well-supported by the provided context, with no significant hallucinations or unsupported information."
      },
      "report": "# Research Report: Evolution of CRISPR-Cas9 Technology Since 2020 and Key Regulatory and Ethical Debates\n\n## Executive Summary\n\nSince 2020, CRISPR-Cas9 technology has undergone significant advancements, expanding its applications in therapeutic and biomedical fields. As of early 2025, approximately 250 clinical trials are underway, reflecting the growing interest and investment in gene-editing therapies. However, the rapid evolution of this technology has sparked critical regulatory and ethical debates, particularly concerning safety, efficacy, and the implications of germline editing. This report synthesizes the latest advancements in CRISPR-Cas9 technology, highlights the current landscape of clinical trials, and discusses the key regulatory and ethical challenges that accompany its clinical applications.\n\n## Key Findings\n\n### 1. Advances in CRISPR-Cas9 Technology\n- **Therapeutic Applications**: CRISPR-Cas9 has been increasingly utilized in various therapeutic contexts, including cancer treatment, genetic disorders, and infectious diseases. Recent studies highlight its transformative impact on personalized medicine and the potential for curative therapies (Frontiers, 2024).\n- **Clinical Trials**: As of February 2025, over 250 clinical trials involving CRISPR-based therapies are being monitored, with more than 150 trials focused on gene-editing candidates. This marks a significant increase in the application of CRISPR technology in clinical settings (CRISPR Medicine News, 2025).\n- **Technological Innovations**: Advances in CRISPR technology include improved delivery methods, enhanced specificity, and the development of novel CRISPR systems (e.g., CRISPR-Cas12 and CRISPR-Cas13), which expand the range of potential applications and reduce off-target effects (NIH, 2021).\n\n### 2. Regulatory Landscape\n- **Regulatory Frameworks**: The regulatory landscape for CRISPR technology is evolving, with agencies like the FDA and EMA developing guidelines for gene-editing therapies. These regulations focus on ensuring safety and efficacy while balancing innovation and public health concerns (CBHD, 2023).\n- **Germline Editing**: The debate surrounding germline editing remains contentious. While some advocate for its potential to eradicate genetic diseases, others raise concerns about unintended consequences and ethical implications, leading to calls for stricter regulations or moratoriums on germline modifications (PMC, 2021).\n\n### 3. Ethical Debates\n- **Equity and Access**: The rapid development of CRISPR technology raises questions about equity in access to gene-editing therapies. Concerns exist that these advancements may exacerbate existing health disparities, particularly in low-income populations (PMC, 2021).\n- **Long-term Implications**: Ethical considerations also extend to the long-term implications of gene editing, including the potential for \"designer babies\" and the societal impacts of altering human genetics. The need for public discourse and ethical frameworks is increasingly recognized (CBHD, 2023).\n\n## Conclusion\n\nThe evolution of CRISPR-Cas9 technology since 2020 has been marked by significant advancements in therapeutic applications and a burgeoning landscape of clinical trials. However, these developments are accompanied by critical regulatory and ethical debates that must be addressed to ensure the responsible use of gene-editing technologies. As CRISPR continues to transform the biomedical landscape, stakeholders must engage in ongoing discussions about safety, equity, and the ethical implications of manipulating the human genome. Balancing innovation with ethical considerations will be essential in shaping the future of CRISPR-Cas9 technology and its applications in medicine."
    },
    {
      "question_id": "rq_003",
      "query": "What are the main architectural differences between transformer models and state space models, and what are the tradeoffs for long-context tasks?",
      "system": "flashresearch",
      "time_budget_s": 600,
      "elapsed_time": 26.763569116592407,
      "node_count": 5,
      "findings_count": 25,
      "report_length": 3941,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 80.0,
        "overall": 85.0,
        "explanation": "The report is well-organized with clear sections and logical flow, though it could benefit from more detailed citations and professional formatting. It directly addresses the query about architectural differences and tradeoffs for long-context tasks, providing a comprehensive analysis. However, some claims, particularly about specific model comparisons, lack direct support from the provided context, indicating minor faithfulness issues."
      },
      "report": "## Executive Summary\nThis report examines the architectural differences between transformer models and state space models (SSMs), particularly in the context of long-context tasks. Key findings indicate that SSMs offer significant computational efficiency advantages over transformers, particularly in handling longer sequences, while transformers excel in managing short-range dependencies and providing detailed context recall.\n\n## Key Findings\n- **Computational Complexity**: SSMs exhibit linear O(N) computational complexity, while transformers have quadratic O(N\u00b2) complexity, making SSMs more efficient for long-context tasks ([Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers](https://arxiv.org/abs/2601.01237)).\n- **Context Length Limitations**: Transformers face a hard limit on context length due to their token cache mechanism, which can lead to performance degradation when this limit is exceeded. In contrast, SSMs can manage longer sequences without significant latency ([Characterizing State Space Model (SSM) and SSM-Transformer](https://arxiv.org/html/2507.12442v2)).\n- **Memory Management**: Transformers maintain a \"token cache\" that allows for perfect recall of previously seen elements, which is beneficial for tasks requiring detailed context manipulation. SSMs, however, are designed to efficiently manage long-range dependencies, which can be advantageous in certain applications ([On the Tradeoffs of SSMs and Transformers](https://www.youtube.com/watch?v=yirZ_XucDg8)).\n- **Performance in Long-Context Tasks**: Studies show that SSMs outperform transformers in long-context scenarios, as evidenced by benchmarking studies comparing models like Mamba (an SSM) with LLaMA (a transformer) on dyadic therapy sessions ([Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers](https://goatstack.ai/articles/2601.01237)).\n\n## Analysis\nThe architectural differences between transformers and SSMs primarily revolve around their computational efficiency and memory management strategies. Transformers utilize a self-attention mechanism that excels in capturing short-range dependencies and providing detailed context, but this comes at the cost of increased computational complexity and limitations on context length. In contrast, SSMs leverage a more efficient approach to manage long-range dependencies, allowing them to scale better for longer sequences.\n\nHowever, the tradeoffs are notable. While SSMs can handle longer contexts more efficiently, they may not capture short-range dependencies as effectively as transformers. This limitation could impact performance in tasks where detailed context and immediate recall are critical. Additionally, the current literature suggests that while SSMs are promising, they are still being developed and may not yet match the versatility of transformers across all tasks ([The New Wave of Sequence Modeling: A Comparative Analysis of State Space Models and Transformers](https://uplatz.com/blog/the-new-wave-of-sequence-modeling-a-comparative-analysis-of-state-space-models-and-transformers-2/)).\n\n## Conclusion\nThe architectural differences between transformers and state space models highlight significant tradeoffs, particularly for long-context tasks. SSMs offer a compelling alternative to transformers, especially in scenarios requiring efficient handling of long sequences. However, the effectiveness of transformers in managing short-range dependencies and providing detailed context remains a strong advantage. Future research should focus on bridging these gaps, potentially leading to hybrid models that can leverage the strengths of both architectures. Current evidence suggests that while SSMs are a promising avenue for long-context modeling, further exploration is needed to fully understand their capabilities and limitations in comparison to transformers."
    },
    {
      "question_id": "rq_003",
      "query": "What are the main architectural differences between transformer models and state space models, and what are the tradeoffs for long-context tasks?",
      "system": "baseline",
      "time_budget_s": 600,
      "elapsed_time": 9.942548990249634,
      "node_count": 1,
      "findings_count": 5,
      "report_length": 3832,
      "scores": {
        "quality": 85.0,
        "relevance": 90.0,
        "faithfulness": 80.0,
        "overall": 85.0,
        "explanation": "The report is well-structured with clear sections and logical flow, though it could benefit from more detailed citations and professional formatting. It directly addresses the query, providing a comprehensive comparison of Transformers and SSMs, including their trade-offs for long-context tasks. However, some claims, particularly about hybrid models, lack direct support from the provided context, indicating minor faithfulness issues."
      },
      "report": "# Research Report: Architectural Differences Between Transformer Models and State Space Models\n\n## Executive Summary\n\nThis report examines the architectural differences between Transformer models and State Space Models (SSMs), focusing on their performance in long-context tasks. Transformers, characterized by their attention mechanisms, excel in capturing dependencies across sequences but face challenges with efficiency as context length increases. In contrast, SSMs utilize a constant hidden state to model sequences, offering linear efficiency but often at the cost of performance in tasks requiring extensive context. Recent hybrid architectures that combine elements of both models are emerging, promising to leverage the strengths of each while mitigating their weaknesses. This report synthesizes findings from various studies to elucidate the trade-offs involved in selecting between these architectures for long-context applications.\n\n## Key Findings\n\n1. **Architectural Differences**:\n   - **Transformers**: Utilize self-attention mechanisms that allow for the direct modeling of relationships between all tokens in a sequence. This results in quadratic complexity with respect to sequence length, making them less efficient for very long contexts.\n   - **State Space Models (SSMs)**: Employ a constant hidden state to represent sequences, which allows for linear complexity. This architecture is particularly advantageous for tasks requiring the processing of long sequences, as it avoids the computational bottlenecks associated with attention mechanisms.\n\n2. **Performance in Long-Context Tasks**:\n   - Research indicates that Transformers outperform SSMs in tasks that require precise copying from input contexts, as demonstrated in the study \"Repeat After Me: Transformers are Better than State Space Models at Copying from Their Input Context\" (Kempner Institute).\n   - However, SSMs show promise in scenarios where efficiency is paramount, particularly in tasks involving long sequences where the quadratic scaling of Transformers becomes prohibitive.\n\n3. **Emerging Hybrid Architectures**:\n   - Recent developments in hybrid models, such as Attention-SSM combinations, aim to integrate the strengths of both architectures. These hybrids promise improved efficiency while maintaining the attention capacity of Transformers, potentially addressing the limitations of both models in long-context scenarios (as discussed in \"End of Transformers? Attention + State-Space Hybrids in 2025\").\n\n4. **Theoretical Trade-offs**:\n   - The theoretical underpinnings of both models reveal inherent trade-offs. Transformers excel in tasks requiring detailed contextual understanding but suffer from inefficiency with longer sequences. Conversely, SSMs provide a more scalable solution but may lack the nuanced understanding that attention mechanisms afford.\n\n## Conclusion\n\nThe choice between Transformer models and State Space Models hinges on the specific requirements of the task at hand, particularly regarding context length and computational efficiency. While Transformers currently lead in tasks demanding high contextual fidelity, SSMs offer a compelling alternative for long-context applications due to their linear efficiency. The emergence of hybrid architectures presents an exciting avenue for future research, potentially combining the best features of both models to enhance performance across a wider range of tasks. As the field of sequence modeling continues to evolve, understanding these architectural differences and their implications will be crucial for developing more effective models for complex tasks. \n\nIn summary, the ongoing exploration of these architectures will likely shape the future landscape of machine learning, particularly in applications requiring the processing of extensive sequences."
    }
  ],
  "summaries": [
    {
      "system": "flashresearch",
      "time_budget_s": 120,
      "num_questions": 3,
      "avg_quality": 85.0,
      "avg_relevance": 86.66666666666667,
      "avg_faithfulness": 88.33333333333333,
      "avg_overall": 86.66666666666667,
      "avg_node_count": 5,
      "avg_elapsed_time": 25.870924711227417,
      "std_overall": 5.773502691896257
    },
    {
      "system": "baseline",
      "time_budget_s": 120,
      "num_questions": 3,
      "avg_quality": 85.0,
      "avg_relevance": 90.0,
      "avg_faithfulness": 85.0,
      "avg_overall": 86.66666666666667,
      "avg_node_count": 1,
      "avg_elapsed_time": 12.96976407368978,
      "std_overall": 2.8867513459481287
    },
    {
      "system": "flashresearch",
      "time_budget_s": 600,
      "num_questions": 3,
      "avg_quality": 85.0,
      "avg_relevance": 86.66666666666667,
      "avg_faithfulness": 76.66666666666667,
      "avg_overall": 82.77777777777777,
      "avg_node_count": 8.666666666666666,
      "avg_elapsed_time": 31.887067715326946,
      "std_overall": 2.5458753860865775
    },
    {
      "system": "baseline",
      "time_budget_s": 600,
      "num_questions": 3,
      "avg_quality": 85.0,
      "avg_relevance": 90.0,
      "avg_faithfulness": 85.0,
      "avg_overall": 86.66666666666667,
      "avg_node_count": 1,
      "avg_elapsed_time": 14.017312129338583,
      "std_overall": 2.8867513459481287
    }
  ],
  "config": {
    "time_budgets": [
      120,
      600
    ],
    "use_paper_models": false,
    "judge_model": "gpt-4o",
    "num_questions": 3
  },
  "formatted_table": "# Table 1: Performance on DeepResearchGym\n\n| System | Time Budget | Quality | Relevance | Faithfulness | Overall | Nodes |\n|--------|-------------|---------|-----------|--------------|---------|-------|\n| flashresearch  | 2min        | 85.0 | 86.7 | 88.3 | 86.7\u00b15.8 | 5.0 |\n| baseline       | 2min        | 85.0 | 90.0 | 85.0 | 86.7\u00b12.9 | 1.0 |\n| flashresearch  | 10min       | 85.0 | 86.7 | 76.7 | 82.8\u00b12.5 | 8.7 |\n| baseline       | 10min       | 85.0 | 90.0 | 85.0 | 86.7\u00b12.9 | 1.0 |\n\n_Based on 3 questions._"
}